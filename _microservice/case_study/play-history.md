# 播放历史

[TOC]



### 需求描述

- 每个用户需要每个几分钟汇报一次播放记录，同时播放每个视频都需要查询一下播放历史记录。
- 用户每次是否第一次观看。
- 按照时间线返回 topN 记录，点查获取进度信息。
- 平台化，视频，文章，漫画等多业务的历史记录。
- 记录删除，清空。

### 分析

这个功能看似没什么复杂的地方，但是历史记录类型的业务，是一个极高 tps 写入，高 qps 读取的业务服务。

高并发的读写，单个记录按时间覆盖，对于数据的一致性，安全性要求也不是及其严格，哪怕出现少部分的丢失不一致也影响不大。

同一个用户同一个视频，的新的记录会覆盖旧的记录。这里需要实现同一个用户的 `顺序一致性`，当然也未必需要太多严格，反正我是经常看到某视频网站播放记录经常乱掉，回退什么的。

所以这个业务最核心的是如何抗住高并发的读写。



### 架构设计

如何抗住大并发量的读写？

**cahche & mq** ：在架构设计中，缓存和消息队列的设计，是抗住流量和流量削峰首要考虑的方案。

**隔离与拆分**：隔离, 常见业务闭环的拆分隔离。对于大流量和小流量的同样可以查分，免得受影响。比如这里 使用独立 BFF 。

在 BFF 层实现业务的编排，数据组装，下层的 service 专注于 数据的读写。



**怎么抗住大量的读写请求？**

- 优先考虑缓存，消息队列
- 批量处理，异步化处理
- 请求合并
- 牺牲一定的一致性，甚至是部分数据安全性，实现最终一致性
- 减少网络请求，往返，将部分远端请求转化为节点本机的实现，



**方向**：先写  redis ，然后批量合并发到 kafka，job 消费 kafka 写入 hbase。 

- 这里用到了类似 *write-back* 的思路，就像 cpu 写内存要先写 cacheline 。
- 缓存无处不在，尤其是在解决不同媒介之间的速度差异，至关重要。网络，磁盘，内存，CPU 都离不开缓存。

无论是 redis 还是 kafka 其实都有着不错的吞吐能力。面对海量的流量，都需要做分布式去中心化的部署设计突破单机限制灵活扩容。



**kafka**

- kafka 的高吞吐，主要依赖与顺序磁盘IO，批量处理，多分区扩展并发读写。
- 为了更好的利用 kafka 的吞吐能力，做`批量打包`处理，在内存中定时定量的聚合用户最后一次播放记录，批量发送给kafka，这里只发送播放记录的用户 id ，视频id 之类的数据。详细信息不放松。
-  按照用户 id 来 sharding，同一个用户的数据只发送到一个分区，保持同一用户的数据顺序一致性。sharding 算法最简单的莫过于直接根据分区数取模，或者使用一致性哈希来寻址。
- 使用 kafka 还起到了存储消费服务和 history-service 解耦的作用， 其实他的作用就是通知消费 job 哪些数据该落库了。

**kafka 消费 job**

- 从 kafka 消费数据，解析出 id ，根据 id 从 redis 中获取对应数据的详情，可以使用 redis 的 mget 批量获取。这样可以减少传输。



### 存储设计

播放历史的场景，没有涉及关系型数据中的复杂查询和事务。这里更注重的是读写性能，分布式能力。

MongoDB， HBase 都可以满足需求。

只需要根据用户 id 查询播放历史即可。具体详细信息，直接 通过 pb 序列化到 value 中就可以了。

用户历史，只保留最近一段时间的即可，太久远也没太多意义。可以给数据加上 TTL 生存时间，无论是 mongo 还是 hbase 都是支持这个功能的。

对于需要做一些大数据分析的事情，可以同步给数据仓库。

对于存储 sharding ，根据用户需要尽可能的均匀分析，避免读写热点问题。比如 rowkey 使用用户 id md5 以后的头两位+用户。



### 缓存及数据结构

每日第一次之类的需求，常见方案 bitmap ，bloom filter。

bitmap 对于稀疏的数据存储，会浪费内存。可以用 roaring bitmap 可以理解为压缩位图 ，它的设计非常巧妙了，而且非常节省内存，可以实现很多有序集合的操作，而且很快。

bloom filter 也能解决是否存在的问题，但是会有误差。可以多分几个分组，具体讲就是先根据用户进行哈希计算分组，最简单的比如取模，然后在分组里再进行 bloom filter 操作，这样可以减少碰撞的问题发生。



### 可用性设计

播放历史的，只要把缓存和消息队列方案做好能抗住流量，基本可以解决一大半的问题。

需要注意的，

- 在 history-service 服务中聚合数据，如果服务重启，会出现丢数据的情况。
- redis cluster 集群主从异步复制，也可能出现丢数据的情况。

但是对于这个业务场景，这是可接受的。



### 其他思考

如何减少优化内网的流量？





